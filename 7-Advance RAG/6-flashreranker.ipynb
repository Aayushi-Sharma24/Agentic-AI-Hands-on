{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d52a84c",
   "metadata": {},
   "source": [
    "**Model Options:**\n",
    "- **Nano**: ~4MB, blazing fast model with competitive performance (ranking precision).\n",
    "- **Small**: ~34MB, slightly slower with the best performance (ranking precision).\n",
    "- **Medium**: ~110MB, slower model with the best zero-shot performance (ranking precision).\n",
    "- **Large**: ~150MB, slower model with competitive performance (ranking precision) for 100+ languages.\n",
    "\n",
    "\n",
    " **Flash Rank**: Ultra-lite & Super-fast Python library for search & retrieval re-ranking.\n",
    "\n",
    "- **Ultra-lite**: No heavy dependencies. Runs on CPU with a tiny ~4MB reranking model.\n",
    "- **Super-fast**: Speed depends on the number of tokens in passages and query, plus model depth.\n",
    "- **Cost-efficient**: Ideal for serverless deployments with low memory and time requirements.\n",
    "- **Based on State-of-the-Art Cross-encoders**: Includes models like ms-marco-TinyBERT-L-2-v2 (default), ms-marco-MiniLM-L-12-v2, rank-T5-flan, and ms-marco-MultiBERT-L-12.\n",
    "- **Sleek Models for Efficiency**: Designed for minimal overhead in user-facing scenarios.\n",
    "\n",
    "_Flash Rank is tailored for scenarios requiring efficient and effective reranking, balancing performance with resource usage._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd65ebf",
   "metadata": {},
   "source": [
    "# Flash Rank using 'flashrank' package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3283041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {i+1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n",
    "                for i, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d399541",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to speedup LLMs?\"\n",
    "\n",
    "passages = [\n",
    "   {\n",
    "      \"id\":1,\n",
    "      \"text\":\"Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.\",\n",
    "      \"meta\": {\"additional\": \"info1\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":2,\n",
    "      \"text\":\"LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper\",\n",
    "      \"meta\": {\"additional\": \"info2\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":3,\n",
    "      \"text\":\"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
    "      \"meta\": {\"additional\": \"info3\"}\n",
    "\n",
    "   },\n",
    "   {\n",
    "      \"id\":4,\n",
    "      \"text\":\"Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.\",\n",
    "      \"meta\": {\"additional\": \"info4\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":5,\n",
    "      \"text\":\"vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels\",\n",
    "      \"meta\": {\"additional\": \"info5\"}\n",
    "   }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57bd4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrank.Ranker import Ranker, RerankRequest\n",
    "\n",
    "def get_result(query,passages,choice):\n",
    "  if choice == \"Nano\":\n",
    "    ranker = Ranker()\n",
    "  elif choice == \"Small\":\n",
    "    ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/opt\")\n",
    "  elif choice == \"Medium\":\n",
    "    ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=\"/opt\")\n",
    "  elif choice == \"Large\":\n",
    "    ranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=\"/opt\")\n",
    "  rerankrequest = RerankRequest(query=query, passages=passages)\n",
    "  results = ranker.rerank(rerankrequest)\n",
    "  print(results)\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72d09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-TinyBERT-L-2-v2...\n",
      "ms-marco-TinyBERT-L-2-v2.zip: 100%|██████████| 3.26M/3.26M [00:01<00:00, 2.04MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 4, 'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.', 'meta': {'additional': 'info4'}, 'score': np.float32(0.018163264)}, {'id': 5, 'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels', 'meta': {'additional': 'info5'}, 'score': np.float32(0.013987866)}, {'id': 3, 'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\", 'meta': {'additional': 'info3'}, 'score': np.float32(0.00091874925)}, {'id': 1, 'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.', 'meta': {'additional': 'info1'}, 'score': np.float32(0.00076141005)}, {'id': 2, 'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper', 'meta': {'additional': 'info2'}, 'score': np.float32(0.0002851765)}]\n",
      "CPU times: total: 703 ms\n",
      "Wall time: 2.75 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': np.float32(0.018163264)},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': np.float32(0.013987866)},\n",
       " {'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': np.float32(0.00091874925)},\n",
       " {'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': np.float32(0.00076141005)},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': np.float32(0.0002851765)}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_result(query,passages,\"Nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ec20df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MiniLM-L-12-v2...\n",
      "ms-marco-MiniLM-L-12-v2.zip: 100%|██████████| 21.6M/21.6M [00:15<00:00, 1.46MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 4, 'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.', 'meta': {'additional': 'info4'}, 'score': np.float32(0.9439003)}, {'id': 3, 'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\", 'meta': {'additional': 'info3'}, 'score': np.float32(0.8475677)}, {'id': 1, 'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.', 'meta': {'additional': 'info1'}, 'score': np.float32(0.8068305)}, {'id': 5, 'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels', 'meta': {'additional': 'info5'}, 'score': np.float32(0.5689538)}, {'id': 2, 'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper', 'meta': {'additional': 'info2'}, 'score': np.float32(0.0044068173)}]\n",
      "CPU times: total: 3.03 s\n",
      "Wall time: 17.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': np.float32(0.9439003)},\n",
       " {'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': np.float32(0.8475677)},\n",
       " {'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': np.float32(0.8068305)},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': np.float32(0.5689538)},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': np.float32(0.0044068173)}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_result(query,passages,\"Small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b6eca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading rank-T5-flan...\n",
      "rank-T5-flan.zip: 100%|██████████| 73.7M/73.7M [00:57<00:00, 1.35MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 3, 'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\", 'meta': {'additional': 'info3'}, 'score': np.float32(0.52268475)}, {'id': 4, 'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.', 'meta': {'additional': 'info4'}, 'score': np.float32(0.50876576)}, {'id': 1, 'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.', 'meta': {'additional': 'info1'}, 'score': np.float32(0.5004556)}, {'id': 2, 'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper', 'meta': {'additional': 'info2'}, 'score': np.float32(0.49322298)}, {'id': 5, 'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels', 'meta': {'additional': 'info5'}, 'score': np.float32(0.37407342)}]\n",
      "CPU times: total: 7.36 s\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': np.float32(0.52268475)},\n",
       " {'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': np.float32(0.50876576)},\n",
       " {'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': np.float32(0.5004556)},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': np.float32(0.49322298)},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': np.float32(0.37407342)}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_result(query,passages,\"Medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc680924",
   "metadata": {},
   "source": [
    "# FlashRank using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d505d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da216cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "loader = PyPDFLoader(\"state_of_union.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "retriever = FAISS.from_documents(texts, embedding).as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3df0ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "alliance last year, and just this morning, Sweden officially joined, and their minister\n",
      "is here tonight. Stand up. Welcome. Welcome, welcome, welcome. And they know\n",
      "how to fight.\n",
      "Mr. Prime Minister, welcome to NATO, the strongest military alliance the world\n",
      "has ever seen.\n",
      "I say this to Congress: We have to stand up to Putin. Send me a bipartisan national\n",
      "security bill. History is literally watching. History is watching. If the United States\n",
      "Metadata: {'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "whatever the hell you want. That’s a quote. A former president actually said that,\n",
      "bowing down to a Russian leader. I think it’s outrageous, it’s dangerous, and it’s\n",
      "unacceptable.\n",
      "America is a founding member of NATO, the military alliance of democratic\n",
      "nations created after World War II to prevent, to prevent war and keep the peace.\n",
      "And today, we’ve made NATO stronger than ever. We welcomed Finland to the\n",
      "alliance last year, and just this morning, Sweden officially joined, and their minister\n",
      "Metadata: {'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "American soldiers at war in Ukraine, and I’m determined to keep it that way.\n",
      "But now, assistance to Ukraine is being blocked by those who want to walk away\n",
      "from our world leadership. Wasn’t long ago when a Republican president named\n",
      "Ronald Reagan thundered, “Mr. Gorbachev, tear down this wall.”\n",
      "Now, now my predecessor, a former Republican president, tells Putin, quote, do\n",
      "whatever the hell you want. That’s a quote. A former president actually said that,\n",
      "Metadata: {'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "3/8/24, 6:43 PM Biden’s 2024 State of the Union Address: Read the Full Transcript - The New York Times\n",
      "https://www.nytimes.com/2024/03/08/us/politics/state-of-the-union-transcript-biden.html 2/26\n",
      "What makes our moment rare is that freedom and democracy are under attack\n",
      "both at home and overseas at the very same time. Overseas, Putin of Russia is on\n",
      "the march, invading Ukraine and sowing chaos throughout Europe and beyond. If\n",
      "Metadata: {'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "security bill. History is literally watching. History is watching. If the United States\n",
      "walks away, it will put Ukraine at risk. Europe is at risk. The free world will be at\n",
      "risk, emboldening others to what they wish to do us harm.\n",
      "Metadata: {'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "the march, invading Ukraine and sowing chaos throughout Europe and beyond. If\n",
      "anybody in this room thinks Putin will stop at Ukraine, I assure you, he will not.\n",
      "But Ukraine, Ukraine can stop Putin. Ukraine can stop Putin, if we stand with\n",
      "Ukraine and provide the weapons they need to defend itself. That is all — that is all\n",
      "Ukraine is asking. They’re not asking for American soldiers. In fact, there are no\n",
      "American soldiers at war in Ukraine, and I’m determined to keep it that way.\n",
      "Metadata: {'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "President Biden delivered his annual State of the Union address on Thursday to a\n",
      "joint session of Congress. The following is a transcript of his remarks, as recorded by\n",
      "The New York Times.\n",
      "Good evening. Good evening. If I were smart, I would go home now.\n",
      "Mr. Speaker, Madam Vice President, members of Congress, my fellow Americans,\n",
      "in January 1941, Franklin Roosevelt came to this chamber to speak to the nation,\n",
      "and he said, “I address you in a moment, unprecedented in the history of the\n",
      "union.”\n",
      "Metadata: {'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 0, 'page_label': '1'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "3/8/24, 6:43 PM Biden’s 2024 State of the Union Address: Read the Full Transcript - The New York Times\n",
      "https://www.nytimes.com/2024/03/08/us/politics/state-of-the-union-transcript-biden.html 24/26\n",
      "And keep our truly sacred obligation, to train and equip those we send into harm’s\n",
      "way and care for them and their families when they come home, and when they\n",
      "don’t. That’s why with the strong support and help of Denis from the V.A., I signed\n",
      "Metadata: {'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 23, 'page_label': '24'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "3/8/24, 6:43 PM Biden’s 2024 State of the Union Address: Read the Full Transcript - The New York Times\n",
      "https://www.nytimes.com/2024/03/08/us/politics/state-of-the-union-transcript-biden.html 8/26\n",
      "A great comeback story is Belvidere, Ill., home to an auto plant for nearly 60 years.\n",
      "Before I came to office the plant was on its way to shutting down. Thousands of\n",
      "workers feared for their livelihoods. Hope was fading.\n",
      "Then I was elected to office and we raised the Belvidere repeatedly with auto\n",
      "Metadata: {'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 7, 'page_label': '8'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "over a decade.\n",
      "And we’re standing up against China’s unfair economic practices. We’re standing\n",
      "up for peace and stability across the Taiwan Straits.\n",
      "I’ve revitalized our partnerships and alliance in the Pacific. India. Australia. Japan.\n",
      "South Korea. Pacific islands. I’ve made sure that the most advanced American\n",
      "technologies can’t be used in China, not allowing to trade them there.\n",
      "Frankly, for all his tough talk on China, it never occurred to my predecessor to do\n",
      "any of that.\n",
      "Metadata: {'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 22, 'page_label': '23'}\n"
     ]
    }
   ],
   "source": [
    "query = \"Which two countries recently joined NATO, according to the speech?\"\n",
    "docs = retriever.invoke(query)\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14e5af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "compressor = FlashrankRerank()\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e84b1c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 1, 'relevance_score': np.float32(0.9981812), 'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}, page_content='whatever the hell you want. That’s a quote. A former president actually said that,\\nbowing down to a Russian leader. I think it’s outrageous, it’s dangerous, and it’s\\nunacceptable.\\nAmerica is a founding member of NATO, the military alliance of democratic\\nnations created after World War II to prevent, to prevent war and keep the peace.\\nAnd today, we’ve made NATO stronger than ever. We welcomed Finland to the\\nalliance last year, and just this morning, Sweden officially joined, and their minister'),\n",
       " Document(metadata={'id': 0, 'relevance_score': np.float32(0.99701995), 'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}, page_content='alliance last year, and just this morning, Sweden officially joined, and their minister\\nis here tonight. Stand up. Welcome. Welcome, welcome, welcome. And they know\\nhow to fight.\\nMr. Prime Minister, welcome to NATO, the strongest military alliance the world\\nhas ever seen.\\nI say this to Congress: We have to stand up to Putin. Send me a bipartisan national\\nsecurity bill. History is literally watching. History is watching. If the United States'),\n",
       " Document(metadata={'id': 2, 'relevance_score': np.float32(0.9961361), 'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}, page_content='American soldiers at war in Ukraine, and I’m determined to keep it that way.\\nBut now, assistance to Ukraine is being blocked by those who want to walk away\\nfrom our world leadership. Wasn’t long ago when a Republican president named\\nRonald Reagan thundered, “Mr. Gorbachev, tear down this wall.”\\nNow, now my predecessor, a former Republican president, tells Putin, quote, do\\nwhatever the hell you want. That’s a quote. A former president actually said that,')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.invoke(\"Which two countries recently joined NATO, according to the speech?\")\n",
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e54e8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "whatever the hell you want. That’s a quote. A former president actually said that,\n",
      "bowing down to a Russian leader. I think it’s outrageous, it’s dangerous, and it’s\n",
      "unacceptable.\n",
      "America is a founding member of NATO, the military alliance of democratic\n",
      "nations created after World War II to prevent, to prevent war and keep the peace.\n",
      "And today, we’ve made NATO stronger than ever. We welcomed Finland to the\n",
      "alliance last year, and just this morning, Sweden officially joined, and their minister\n",
      "Metadata: {'id': 1, 'relevance_score': np.float32(0.9981812), 'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "alliance last year, and just this morning, Sweden officially joined, and their minister\n",
      "is here tonight. Stand up. Welcome. Welcome, welcome, welcome. And they know\n",
      "how to fight.\n",
      "Mr. Prime Minister, welcome to NATO, the strongest military alliance the world\n",
      "has ever seen.\n",
      "I say this to Congress: We have to stand up to Putin. Send me a bipartisan national\n",
      "security bill. History is literally watching. History is watching. If the United States\n",
      "Metadata: {'id': 0, 'relevance_score': np.float32(0.99701995), 'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "American soldiers at war in Ukraine, and I’m determined to keep it that way.\n",
      "But now, assistance to Ukraine is being blocked by those who want to walk away\n",
      "from our world leadership. Wasn’t long ago when a Republican president named\n",
      "Ronald Reagan thundered, “Mr. Gorbachev, tear down this wall.”\n",
      "Now, now my predecessor, a former Republican president, tells Putin, quote, do\n",
      "whatever the hell you want. That’s a quote. A former president actually said that,\n",
      "Metadata: {'id': 2, 'relevance_score': np.float32(0.9961361), 'producer': 'Skia/PDF m122', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36', 'creationdate': '2024-03-08T23:43:36+00:00', 'moddate': '2024-03-08T23:43:36+00:00', 'source': 'state_of_union.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94acd6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which two countries recently joined NATO, according to the speech?',\n",
       " 'result': 'According to the speech, Finland and Sweden recently joined NATO.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1ccd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
